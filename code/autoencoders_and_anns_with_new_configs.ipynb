{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from detector import *\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__[0] != '2':\n",
    "    raise ImportError('This notebook requires Tensorflow v2.')\n",
    "    \n",
    "#verify gpu is avaiable\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack = 'CarliniLInfMethodinf'\n",
    "attack = 'ProjectedGradientDescentinf'\n",
    "\n",
    "activation = 'sigmoid'\n",
    "# activation = 'tanh'\n",
    "\n",
    "# load_benign = True\n",
    "load_benign = False\n",
    "\n",
    "load_adv = True\n",
    "# load_adv = False\n",
    "\n",
    "# train_autoencoders = True\n",
    "train_autoencoders = False\n",
    "\n",
    "# train_anns = True\n",
    "train_anns = False\n",
    "\n",
    "# compute_probs = True\n",
    "compute_probs = False\n",
    "\n",
    "pred_train_or_test = 'test'\n",
    "# pred_train_or_test = 'train'\n",
    "\n",
    "# predict_benign = True\n",
    "predict_benign = False\n",
    "\n",
    "predict_adv = True\n",
    "# predict_adv = False\n",
    "\n",
    "evaluation = True\n",
    "# evaluation = False\n",
    "\n",
    "EXT = '.npy'\n",
    "\n",
    "folder = './'+attack+'/'+activation+'/'\n",
    "LAYER = 21\n",
    "PARTS = 17\n",
    "# TRAIN_SAMPLES = int(8313 * 0.9)\n",
    "# TEST_SAMPLES = 8313 - TRAIN_SAMPLES\n",
    "# ALL_SAMPLES = 8313\n",
    "\n",
    "data_dir_name = '../../ae_dataset loader/src/logits/vgg19/'+attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAutoEncoderAndTrain(x_train):\n",
    "    input_shape = x_train[0].flatten().shape[0]\n",
    "    input_img = Input(shape=(input_shape,))\n",
    "    encoded = BatchNormalization()(input_img)\n",
    "    encoded = Dense(160, activation=activation)(encoded)\n",
    "    encoded = Dense(100, activation=activation)(encoded)\n",
    "\n",
    "    decoded = Dense(160, activation=activation)(encoded)\n",
    "    decoded = Dense(input_shape, activation=activation)(decoded)\n",
    "    \n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    autoencoder.fit(x_train, x_train,\n",
    "                epochs=45,\n",
    "                batch_size=512,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.1,                   \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "                   )\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_logits(benign_or_adv):\n",
    "    logits = []\n",
    "    for i in range(LAYER):\n",
    "            layer_logits = []\n",
    "            count = 0\n",
    "            # print('[{:11.2f}s][+] train activation space: layer {}/21'.format(time.time() - start, i + 1))\n",
    "            for j in range(PARTS):\n",
    "                f_x_train = np.load(os.path.join(data_dir_name, benign_or_adv, 'layer_{}_{}'.format(i, j) + EXT))\n",
    "                flatten_shape = 1\n",
    "    #             print(f_x_train.shape)\n",
    "                for shape_index in range(1, len(f_x_train.shape)):\n",
    "                    flatten_shape = flatten_shape * f_x_train.shape[shape_index]\n",
    "                f_x_train = f_x_train.flatten().reshape(f_x_train.shape[0], flatten_shape)\n",
    "    #             print(f_x_train.shape)\n",
    "                layer_logits.append(f_x_train)\n",
    "            layer_logits = np.array(layer_logits)\n",
    "            layer_logits = np.concatenate(layer_logits, axis=0)\n",
    "            logits.append(layer_logits)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_benign:\n",
    "    benign_logits = load_logits('benign')\n",
    "    logits = benign_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_adv:\n",
    "    adv_logits = load_logits('adversarial')\n",
    "    logits = adv_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_benign or load_adv:\n",
    "    n_layers = len(logits)\n",
    "    n_samples = logits[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4952 1595 2952 ... 5452 2714 5994]\n",
      "(7833,)\n",
      "(871,)\n"
     ]
    }
   ],
   "source": [
    "#extract train test\n",
    "n_train_samples = int(n_samples * 0.9)\n",
    "n_test_samples = n_samples - n_train_samples\n",
    "\n",
    "# random.seed(os.urandom(5))\n",
    "seed = 1337\n",
    "all_indexes = np.arange(0, n_samples, 1)\n",
    "random.Random(seed).shuffle(all_indexes)\n",
    "train_indexes = all_indexes[:n_train_samples]\n",
    "test_indexes = all_indexes[n_train_samples:]\n",
    "print(all_indexes)\n",
    "print(train_indexes.shape)\n",
    "print(test_indexes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train autoencoders\n",
    "if train_autoencoders:\n",
    "    for layer_index in range(n_layers):\n",
    "        print('layer: ', layer_index)\n",
    "        layer_encoder= createAutoEncoderAndTrain((benign_logits[layer_index])[train_indexes])\n",
    "        layer_encoder.save(folder+'autoencoders/layer_'+str(layer_index)+'.hdf5')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7833, 10)\n",
      "(871, 10)\n"
     ]
    }
   ],
   "source": [
    "y = np.load(os.path.join(data_dir_name, 'labels_benign.npy'))\n",
    "y_hot = to_categorical(y)\n",
    "y_train = y_hot[train_indexes]\n",
    "y_test = y_hot[test_indexes]\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndTrainAnn(x_train, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x_train[0].shape[0], activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train,\n",
    "              y_train,\n",
    "              epochs=70,\n",
    "              batch_size=512,\n",
    "              shuffle=True,\n",
    "              validation_split = 0.1,\n",
    "              callbacks = [tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "             )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_from_autoencoder(autoencoder, data):\n",
    "    model = autoencoder\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[3].output)\n",
    "    intermediate_output = intermediate_layer_model.predict(data, batch_size=512)\n",
    "    return intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and save ann\n",
    "if train_anns:\n",
    "    layers_ann = []\n",
    "    train_layers_predictions = []\n",
    "    for layer_index in range(n_layers):\n",
    "        x_train = (benign_logits[layer_index])[train_indexes]\n",
    "        filepath = folder+'autoencoders/layer_'+str(layer_index)+'.hdf5'\n",
    "        layer_autoencoder = tf.keras.models.load_model(filepath)\n",
    "        encoded = encode_from_autoencoder(layer_autoencoder, x_train)\n",
    "        print('layer number: ', layer_index)\n",
    "        layer_ann = createAndTrainAnn(encoded, y_train)\n",
    "        layer_ann.save(folder+'anns/layer_'+str(layer_index)+'.hdf5') \n",
    "        layers_ann.append(layer_ann)\n",
    "    #     train_layers_predictions.append(tf.argmax(layer_ann.predict(encoded, batch_size=512), axis=0))\n",
    "else:\n",
    "    #load anns\n",
    "    layers_ann = []\n",
    "    train_layers_predictions = []\n",
    "    for layer_index in range(n_layers):\n",
    "        layer_ann = tf.keras.models.load_model(folder+'anns/layer_'+str(layer_index)+'.hdf5')\n",
    "        layers_ann.append(layer_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers_predictions(layers_ann, logits, layer_indexes):\n",
    "    layers_predictions = []\n",
    "    for layer_index in range(n_layers):\n",
    "        x_train = (logits[layer_index])[layer_indexes]\n",
    "        filepath = folder+'autoencoders/layer_'+str(layer_index)+'.hdf5'\n",
    "        layer_autoencoder = tf.keras.models.load_model(filepath)\n",
    "        encoded = encode_from_autoencoder(layer_autoencoder, x_train)\n",
    "#         print(encoded.shape)\n",
    "        layers_predictions.append(tf.argmax(layers_ann[layer_index].predict(encoded, batch_size=512), axis=1))\n",
    "    return layers_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_probs:\n",
    "    train_layers_predictions = get_layers_predictions(layers_ann, benign_logits, train_indexes)\n",
    "\n",
    "    layer_prob = []\n",
    "    for layer_index in range(1, n_layers):\n",
    "        cur_layer_pred = train_layers_predictions[layer_index]\n",
    "        prev_layer_pred =train_layers_predictions[layer_index-1]\n",
    "        count_diff = tf.reduce_sum(tf.cast(cur_layer_pred != prev_layer_pred, tf.float32))\n",
    "        layer_prob.append(count_diff / train_layers_predictions[layer_index].shape[0])\n",
    "        print(count_diff)\n",
    "    tf.print(layer_prob)\n",
    "    np.save(folder+'layer_prob.npy', layer_prob)\n",
    "else:\n",
    "    layer_prob = np.load(folder+'layer_prob.npy')\n",
    "    layer_prob+= float(1e-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0.36128488931665065, 0.36453320500481234, 0.23472088546679498, 0.21246390760346487, 0.1347449470644851, 0.10190086621751684, 0.021896053897978825, 0.001203079884504331, 1e-38, 1e-38, 1e-38, 1e-38, 1e-38, 1e-38, 1e-38, 1e-38, 1e-38, 1e-38, 1e-38, 1e-38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44274226\n",
      "0.4589557\n",
      "0.5818971\n",
      "0.6336014\n",
      "0.45793438\n",
      "0.743138\n",
      "0.56044936\n",
      "0.1940508\n",
      "0.054002296\n",
      "0.055917274\n",
      "0.036767524\n",
      "0.010340866\n",
      "0.000127665\n",
      "1e-32\n",
      "0.000127665\n",
      "0.000127665\n",
      "1e-32\n",
      "1e-32\n",
      "1e-32\n",
      "0.00076599006\n"
     ]
    }
   ],
   "source": [
    "# layer_prob += [float(1e-32)\n",
    "for prob in layer_prob:\n",
    "    tf.print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loglikelihood(predictions, layer_prob):\n",
    "    log_likeligood = np.zeros(predictions[0].shape[0])\n",
    "    for layer_index in range(1, len(predictions)):\n",
    "        cur_layer_pred = predictions[layer_index]\n",
    "        prev_layer_pred = predictions[layer_index-1]\n",
    "        pred_changed = tf.cast(cur_layer_pred != prev_layer_pred, tf.float32)\n",
    "        changed_indexes = pred_changed == 1\n",
    "        same_indexes = pred_changed == 0\n",
    "        log_likeligood[changed_indexes]+= np.full((np.sum(changed_indexes)), tf.math.log(layer_prob[layer_index-1]))        \n",
    "        \n",
    "        log_likeligood[same_indexes]+= np.full((np.sum(same_indexes)), tf.math.log(1-layer_prob[layer_index-1]))\n",
    "    print(log_likeligood)\n",
    "    return log_likeligood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(pred_train_or_test == 'train'):\n",
    "    indexes = train_indexes\n",
    "else:\n",
    "    indexes = test_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_test = pred_train_or_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ProjectedGradientDescentinf/sigmoid/test'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder+train_or_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_benign:\n",
    "    benign_pred = get_layers_predictions(layers_ann, benign_logits, indexes)\n",
    "    benign_log = get_loglikelihood(benign_pred, layer_prob)\n",
    "    np.save(folder+train_or_test+'_benign_log.npy', benign_log)\n",
    "else:\n",
    "    benign_log = np.load(folder+train_or_test+'_benign_log.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -6.47910809   -4.79209944   -6.09742825   -6.42901884   -5.51729112\n",
      "   -5.33568404   -5.78894966  -93.31838386   -7.04830298   -6.04998167\n",
      "   -4.94109654   -6.03193562   -5.10564397   -6.23612213   -7.50842135\n",
      "   -5.08355662   -4.94109654   -5.74733119   -6.57963397   -5.02217774\n",
      "  -17.61839634   -5.74733119   -4.22737966   -5.41676524   -6.06746094\n",
      "   -6.41508654   -4.06283223   -4.47447938   -8.03435029   -4.63902682\n",
      "  -13.35127502   -6.52954472   -5.1711366    -6.75958479   -4.80504534\n",
      "   -4.94109654   -4.94109654   -6.45898615   -6.1284202   -17.22380884\n",
      "   -4.80093158   -5.33568404   -5.70136967   -4.80504534  -95.53717541\n",
      "   -5.33568404  -10.86069086   -6.64512661   -7.45583637   -4.06283223\n",
      "  -10.12864907   -5.0350854   -10.58631334   -6.56668808   -5.51729112\n",
      "   -5.1711366    -6.96072299   -5.92876706   -6.1284202    -4.61053059\n",
      "   -4.94109654   -5.50434522   -4.62608092   -5.45838371   -4.95664687\n",
      "   -5.50434522   -8.91319537   -6.41508654   -8.06902736   -6.96072299\n",
      "  -10.24749987   -5.76274855   -5.35274369   -4.86906688   -8.00353473\n",
      "   -5.53682224   -4.94109654   -5.1840825    -6.36499729   -5.1711366\n",
      "   -6.40214064   -4.62608092  -11.89264678   -4.47036563   -5.02217774\n",
      "  -11.89264678   -5.34862993   -5.1711366    -5.10564397   -5.7668623\n",
      "   -4.62343825   -6.15050756  -11.34494843  -16.07075262   -4.39604085\n",
      "   -5.67287344   -6.67611856  -16.67611049   -5.35274369   -6.26197569\n",
      "   -5.85444229   -5.33568404  -11.97987922   -4.79209944   -7.59600133\n",
      "   -5.70136967   -6.31456066   -4.7266068    -4.06283223   -5.00511808\n",
      "   -4.56205937 -176.59742424   -4.70040569  -23.31358514  -11.72398559\n",
      "   -7.6313859    -5.51729112  -12.28723428  -11.73953593   -6.41508654\n",
      "   -5.1840825    -4.47447938  -13.56576475   -6.6720048    -8.6831553\n",
      "  -10.14697398   -4.94109654   -5.89426638  -14.19193396   -5.01806398\n",
      "   -6.20309253   -4.94109654   -4.63902682   -5.68842377   -4.56205937\n",
      "   -6.80967404 -115.25882112   -5.1711366    -6.53365848   -9.46089372\n",
      "   -6.33664801  -12.45301759   -4.47036563   -7.35531049   -5.57867\n",
      "   -6.07157469   -6.60798325  -11.21343059   -5.00923184  -13.24370191\n",
      "   -6.08452059   -9.43032364  -12.90169452   -4.84057065   -7.00257075\n",
      "   -6.24906802   -5.2522178    -4.46153349   -5.10564397   -5.62440222\n",
      "   -5.35521515   -5.9029135    -6.83802332  -16.983118     -4.45741973\n",
      "   -5.02217774   -4.2928723    -5.88132048   -5.81994161   -8.6831553\n",
      "  -11.17557709   -6.52954472   -6.62353359  -13.78946774   -5.74733119\n",
      " -103.91368001   -4.39604085   -6.31456066   -4.22737966   -4.62608092\n",
      "   -8.74453418   -4.23149342   -6.04998167   -6.23347946   -6.1284202\n",
      "   -4.94373921  -12.54398015   -4.80504534   -4.86642421   -5.89426638\n",
      "   -5.58278375   -4.22737966   -5.89838014   -4.61053059   -5.88132048\n",
      "   -6.47910809   -4.47447938   -4.45741973   -5.18672517   -6.03193562\n",
      "   -5.33568404  -11.62096399   -5.88132048  -13.82165838   -5.10975773\n",
      "   -4.94109654   -6.40214064  -10.86069086   -5.1711366    -5.65128042\n",
      "   -4.94109654  -14.04293686   -8.51860787   -5.12517509  -10.70761824\n",
      "   -6.59503736  -12.52103932   -4.94109654   -5.58278375   -6.42901884\n",
      "   -4.39339818   -5.33568404   -5.62028847   -4.70451945   -6.53365848\n",
      "   -4.62608092   -4.23149342   -5.41676524   -5.68842377   -5.68842377\n",
      "   -5.62440222   -6.36499729   -6.06292757  -11.86165483   -5.53682224\n",
      "  -18.70321609   -4.2928723    -5.57867      -5.81994161  -12.2134142\n",
      "   -5.01806398   -6.96072299   -6.00608206 -182.58522443   -4.3058182\n",
      "  -11.03772217   -6.23612213   -5.84153463  -10.63065079   -4.70040569\n",
      "   -5.9029135    -5.84153463   -4.84057065   -5.33568404   -4.22737966\n",
      "   -6.47910809   -5.1711366    -9.75238649  -11.63323883   -6.42901884\n",
      "   -4.39339818   -5.24810405   -7.19076306   -6.047339     -5.57867\n",
      "   -5.00511808   -4.7266068    -5.34862993   -4.70040569   -4.96959277\n",
      "   -5.68842377   -4.70451945   -6.59503736   -5.58278375   -4.70451945\n",
      "  -10.86069086   -5.27430516   -5.48673299   -5.76274855   -4.22737966\n",
      "   -4.45741973   -4.39604085   -4.63902682   -6.7791159    -5.53682224\n",
      "   -5.68842377  -18.02737744   -5.10975773   -5.83742087  -16.78446949\n",
      "   -6.24906802   -9.53015132   -6.42901884   -5.01806398   -5.74733119\n",
      "   -6.42901884   -6.84066599   -6.00343939   -4.94109654   -6.36499729\n",
      "   -6.59503736   -4.77507802   -5.34862993   -4.23149342   -5.00923184\n",
      "   -6.07157469   -5.1711366    -5.29383627   -4.47447938   -6.19897877\n",
      "   -4.63902682   -6.92824598   -6.7920618   -12.80501216   -6.31456066\n",
      "  -16.23677113   -5.0350854    -4.47447938   -4.06283223   -5.33979779\n",
      "   -6.59503736   -9.24376132   -6.88228446   -4.45741973  -13.89897338\n",
      "   -5.51729112   -5.27430516   -5.65539418   -4.2928723    -6.1284202\n",
      "   -5.35274369  -12.07010188   -4.45741973   -5.74733119   -5.83742087\n",
      "   -4.2928723    -6.01898972   -5.18672517   -6.36763996   -5.1711366\n",
      "   -8.6831553    -4.94109654   -7.35531049   -5.00923184   -4.39604085\n",
      "  -12.71830672   -5.76274855   -8.91319537  -12.59560705   -5.00511808\n",
      "   -6.19897877   -4.7266068    -5.51729112   -4.2928723   -12.28847016\n",
      "   -6.40214064   -5.02217774   -4.70451945   -5.45838371   -9.91693392\n",
      "   -5.08355662   -6.41508654   -4.22737966   -6.21188644   -5.58278375\n",
      "   -4.46153349   -5.50434522   -8.00353473   -4.70451945   -4.94373921\n",
      "   -4.22737966   -5.0350854    -4.70451945   -5.53682224  -15.65344008\n",
      "   -6.03443134   -5.02217774   -5.00511808  -11.8897689    -5.50434522\n",
      "   -6.41097279   -4.61053059   -4.80504534   -5.10975773  -12.05719422\n",
      "  -12.12268685   -6.21188644   -4.79209944   -4.70451945   -5.88543424\n",
      "   -5.71677305  -10.63065079   -5.1840825    -6.54907584   -4.96959277\n",
      "   -5.33568404   -4.23149342   -5.1840825    -6.08452059   -5.41676524\n",
      "   -4.86906688   -5.34862993   -4.62608092   -5.45838371   -6.47910809\n",
      "  -14.99427067   -6.36499729   -5.33568404   -5.86738819   -4.39604085\n",
      "   -6.36499729 -109.00302631  -14.66760266   -4.62608092   -4.06283223\n",
      "   -5.7668623    -5.68842377   -5.88543424   -4.46153349   -4.47447938\n",
      "   -5.1840825    -5.35274369   -5.85444229   -5.7668623    -5.70136967\n",
      "  -17.05514765  -10.3129925    -4.94109654   -5.48673299   -5.24810405\n",
      "   -5.50434522   -5.00511808   -5.70136967   -4.2928723   -27.44965063\n",
      "   -6.94777709   -6.26447141  -12.69623876   -5.1711366   -10.73942419\n",
      "   -5.51729112  -12.0779806    -5.1711366    -4.22737966   -4.22737966\n",
      "   -5.58278375   -6.60798325   -5.51729112   -5.0350854    -5.93140974\n",
      "   -4.61053059  -12.2843564   -94.26601934   -4.94109654   -4.2928723\n",
      "   -6.60798325   -6.00608206   -5.17377927   -5.2522178    -9.80440557\n",
      "   -6.36499729   -5.35274369  -14.54368331   -5.24810405   -6.71773703\n",
      "   -7.50842135   -6.03193562   -6.36499729   -6.75958479   -8.91319537\n",
      "   -4.39339818  -14.48230443   -6.47910809   -5.29383627   -5.29383627\n",
      "   -6.42901884   -4.86906688   -4.77919178   -5.00923184   -6.19897877\n",
      "   -5.18672517   -4.70040569  -10.63065079   -5.65539418   -4.2928723\n",
      "  -88.1097652    -4.46153349   -4.3058182    -4.23149342  -11.88853303\n",
      "   -6.06746094  -11.73953593   -6.83802332   -5.1711366  -176.2289269\n",
      "   -4.95664687   -6.03443134   -5.58278375   -4.2928723    -5.02217774\n",
      "   -4.22737966   -4.84057065   -4.86906688   -4.62608092   -5.70136967\n",
      "  -16.81857056   -5.1840825    -5.33568404  -10.55862114   -4.94109654\n",
      "   -5.57867      -6.20309253  -25.47805152   -6.59503736  -10.21665093\n",
      "   -5.0350854    -4.85351655   -4.70451945  -17.05514765   -9.22821098\n",
      "  -11.04229794  -15.90620518   -6.4331326    -5.1711366    -6.42901884\n",
      "  -11.20226961   -6.29296764   -5.17377927   -4.45741973   -4.7266068\n",
      "   -5.1840825    -5.17377927  -13.40560667   -4.80504534   -5.74733119\n",
      "   -4.7266068    -6.3104469   -11.34494843 -186.55988131   -6.59503736\n",
      "   -4.79209944  -20.0501494    -5.1840825   -20.0501494    -5.88132048\n",
      "   -5.33568404   -5.00923184   -4.56205937  -11.89264678   -4.85351655\n",
      "   -9.29634629   -4.22737966   -9.09215978   -6.23612213  -11.10125232\n",
      "   -5.33568404   -4.56205937   -5.1711366    -6.75958479   -4.45741973\n",
      "   -5.10564397  -12.2973023    -4.86906688  -11.73953593   -6.08452059\n",
      "   -5.0350854    -6.75958479   -4.70451945   -4.45741973   -5.1711366\n",
      "   -4.47447938   -9.46089372   -5.84153463  -12.85160527  -15.04291045\n",
      "   -6.03443134   -4.94373921   -6.38054762  -12.61780023   -5.58278375\n",
      "   -6.3104469    -7.59600133   -5.1711366    -6.80967404   -5.41676524\n",
      "   -5.02217774   -5.57867      -4.63902682   -5.00511808   -4.47447938\n",
      "  -13.65022261   -5.57867      -5.51729112   -4.47447938   -6.59768003\n",
      "   -6.24642535   -5.57867      -4.77919178   -6.42901884   -5.17377927\n",
      "  -11.89264678   -6.36499729   -5.92876706  -10.86674842  -11.90555445\n",
      "   -6.52954472   -5.1711366   -12.68705783   -4.84057065   -6.17210058\n",
      "   -6.00608206   -4.47447938   -5.33979779   -5.24810405   -5.89838014\n",
      "   -5.52387634   -4.06283223   -5.1711366    -5.48673299   -7.26543538\n",
      "   -5.02217774   -5.45838371   -5.93140974   -5.18672517   -4.56205937\n",
      "   -6.23612213   -5.98596013   -5.93140974   -4.62343825   -7.73846141\n",
      "   -7.04830298   -7.29128894   -5.02217774   -5.50434522   -5.1711366\n",
      "   -6.22483233   -7.49547545   -9.39275841   -4.45741973   -5.51729112\n",
      "   -6.09742825   -4.56205937   -9.46089372   -6.80967404  -19.35802555\n",
      "   -6.36499729   -7.80431023   -6.47646542   -4.3058182    -7.21285041\n",
      "   -5.00511808   -5.93140974   -6.21600019   -4.06283223   -4.56205937\n",
      "   -5.41676524   -4.78798568   -4.94109654   -8.44188369   -9.45825105\n",
      "   -4.78798568   -4.39339818   -6.12430645   -5.84153463   -6.41508654\n",
      "  -17.05514765   -5.17377927   -6.75958479   -9.22821098  -13.40972042\n",
      "   -5.78894966   -5.58278375  -93.55230247  -16.6215601    -5.1711366\n",
      "   -5.65128042  -15.56128432  -10.76622254   -5.1711366    -5.41412257\n",
      "   -4.39339818   -6.36499729   -4.86906688   -5.7668623    -6.64512661\n",
      "   -6.14589947  -92.39348502   -7.35531049   -4.22737966   -4.39604085\n",
      "   -4.62608092   -4.53585826   -4.70040569   -6.23612213   -6.06292757\n",
      "   -5.65128042   -5.98596013   -4.45741973   -4.77919178   -5.88132048\n",
      "   -5.93140974   -6.80967404   -4.63902682   -6.52954472  -11.57498849\n",
      "   -4.39604085   -5.57867     -10.58631334   -5.08355662   -5.0350854\n",
      "   -6.64512661   -5.85444229   -5.00923184   -4.63902682   -6.41508654\n",
      "   -7.80431023   -5.93140974   -6.52954472   -6.24642535   -4.62608092\n",
      "  -10.63065079   -5.50434522  -19.72837736  -10.58631334   -4.23149342\n",
      "   -6.92824598   -4.86906688   -6.80967404   -4.70451945   -5.29383627\n",
      "   -5.88543424   -5.89838014 -105.20652494   -8.74864793  -12.45301759\n",
      "   -5.57867      -5.83742087   -6.24906802   -4.06283223   -5.58278375\n",
      "   -5.88132048   -5.9158594    -5.33568404  -12.15494805   -5.1711366\n",
      "   -4.56205937   -4.62608092   -5.93140974   -7.67032611   -8.91319537\n",
      "  -10.0252647    -4.70451945   -4.77507802   -4.45741973   -5.74733119\n",
      "   -4.53585826   -5.51729112   -7.49547545   -5.2522178   -22.99798875\n",
      "   -5.10975773   -4.77507802   -5.02217774   -5.19963283   -5.00511808\n",
      "   -5.62440222   -7.45583637   -6.31456066   -4.47447938   -4.86906688\n",
      "   -5.7668623    -5.98596013  -11.81797446   -4.46153349   -6.59503736\n",
      "   -5.70136967   -6.64512661   -6.41508654  -12.88847839   -4.70451945\n",
      "  -13.12584012   -6.09742825   -4.84057065   -6.08452059   -6.03443134\n",
      " -180.03959365  -16.74013203   -5.59820111   -6.59768003   -4.46153349\n",
      "   -5.34862993   -4.56205937   -4.62608092   -6.31456066   -5.57867\n",
      "   -4.94109654   -5.62440222   -6.21188644  -26.10683108  -20.80777986\n",
      "   -5.83742087   -5.62440222   -4.45741973   -5.33568404   -5.93140974\n",
      "  -12.2134142   -10.55862114   -4.56205937   -6.47910809   -9.75238649\n",
      "   -6.75958479  -12.43746725  -14.72529039 -102.25597422   -4.85351655\n",
      "   -4.79209944   -5.74733119   -4.78798568   -4.53585826   -6.36499729\n",
      "   -5.0350854    -9.47119695   -5.33979779   -4.62608092   -5.00511808\n",
      "   -5.2522178    -5.2522178    -5.84153463   -4.61053059   -4.61053059\n",
      "   -5.02217774   -4.45741973   -5.41412257   -4.23149342   -6.00343939\n",
      "  -10.35868914   -5.08355662  -27.56526592  -14.33439404   -4.77919178\n",
      "   -5.68842377   -5.7668623    -4.47036563   -4.45741973   -4.94109654\n",
      "   -5.00923184   -4.94109654   -4.06283223   -6.56668808   -4.47447938\n",
      "  -10.38995995   -4.45741973   -4.53585826   -6.1284202   -25.72515124\n",
      "   -5.93140974   -8.91086115   -4.77919178   -6.06292757   -5.10975773\n",
      "   -5.41412257   -7.52132901   -6.24906802  -11.17557709   -5.33568404\n",
      "   -5.41676524   -4.45741973   -6.31456066   -5.02217774   -5.02217774\n",
      "  -27.80239037]\n"
     ]
    }
   ],
   "source": [
    "if predict_adv:\n",
    "    adv_pred = get_layers_predictions(layers_ann, adv_logits, indexes)\n",
    "    adv_log = get_loglikelihood(adv_pred, layer_prob)\n",
    "    np.save(folder+train_or_test+'_adv_log.npy', adv_log)\n",
    "else:\n",
    "    adv_log = np.load(folder+train_or_test+'_adv_log.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benign_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_or_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if evaluation:\n",
    "#     logs = np.concatenate((benign_log, adv_log))\n",
    "#     y_logs = [0] * len(benign_log) + [1] * len(adv_log)\n",
    "#     print(len(logs))\n",
    "#     print(len(y_logs))\n",
    "\n",
    "#     fpr, tpr, thresholds = roc_curve(y_logs, logs, pos_label=0)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#     lw = 2\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "#     plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "#     plt.xlim([-0.02, 1.02])\n",
    "#     plt.ylim([-0.02, 1.02])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver Operating Characteristic graph')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "\n",
    "#     plt.savefig(folder+train_or_test+'_roc_auc.png')\n",
    "#     plt.show(block=True)\n",
    "#     plt.clf()\n",
    "\n",
    "#     lr_precision, lr_recall, _ = precision_recall_curve(y_logs, logs, pos_label=0)\n",
    "#     precision_recall_auc = auc(lr_recall, lr_precision)\n",
    "\n",
    "#     lw = 2\n",
    "#     plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "#     plt.plot(lr_recall, lr_precision, color='darkorange', lw=lw, label='precision recall curve (area = %0.3f)' % precision_recall_auc)\n",
    "#     plt.xlim([-0.02, 1.02])\n",
    "#     plt.ylim([-0.02, 1.02])\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.title('Precision Recall graph')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "\n",
    "#     plt.savefig(folder+train_or_test+'_precision_recall_auc.png')\n",
    "#     plt.show(block=True)\n",
    "#     plt.clf()\n",
    "\n",
    "#     plt.figure(figsize=(50,50))\n",
    "#     plt.bar(np.arange(benign_log.shape[0]), benign_log, color='#7f6d5f', label='benign', align='edge')\n",
    "#     plt.bar(np.arange(adv_log.shape[0])+ benign_log.shape[0], adv_log, color='000000', label='adv', align='edge')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(folder+attack+'_'+train_or_test+'_histogram.png')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adv_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742\n",
      "1742\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if evaluation:\n",
    "    logs = np.concatenate((benign_log, adv_log))\n",
    "    y_logs = [0] * len(benign_log) + [1] * len(adv_log)\n",
    "    print(len(logs))\n",
    "    print(len(y_logs))\n",
    "\n",
    "#     fpr, tpr, thresholds = roc_curve(y_logs, logs, pos_label=0)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    lw = 2\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "#     plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "#     plt.xlim([-0.02, 1.02])\n",
    "#     plt.ylim([-0.02, 1.02])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver Operating Characteristic graph')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "\n",
    "#     plt.savefig(folder+train_or_test+'_roc_auc.png')\n",
    "#     plt.show(block=True)\n",
    "#     plt.clf()\n",
    "\n",
    "#     lr_precision, lr_recall, _ = precision_recall_curve(y_logs, logs, pos_label=0)\n",
    "#     precision_recall_auc = auc(lr_recall, lr_precision)\n",
    "\n",
    "#     lw = 2\n",
    "#     plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "#     plt.plot(lr_recall, lr_precision, color='darkorange', lw=lw, label='precision recall curve (area = %0.3f)' % precision_recall_auc)\n",
    "#     plt.xlim([-0.02, 1.02])\n",
    "#     plt.ylim([-0.02, 1.02])\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.title('Precision Recall graph')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "\n",
    "#     plt.savefig(folder+train_or_test+'_precision_recall_auc.png')\n",
    "#     plt.show(block=True)\n",
    "#     plt.clf()\n",
    "    \n",
    "    \n",
    "# ----------------------------------    \n",
    "\n",
    "#     plt.figure(figsize=(50,50))\n",
    "#     plt.bar(np.arange(benign_log.shape[0]), benign_log, color='#7f6d5f', label='benign', align='edge')\n",
    "#     plt.bar(np.arange(adv_log.shape[0])+ benign_log.shape[0], adv_log, color='000000', label='adv', align='edge')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(folder+attack+'_'+train_or_test+'_histogram.png')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.figure(figsize=(15, 15))\n",
    "    plt.xlabel('test sample #')\n",
    "    plt.ylabel('likelihood value')\n",
    "    plt.title('test samples likelihood value')    \n",
    "    plt.ylim([-0.02, 180.00])\n",
    "    plt.axvline(x=len(benign_log), linewidth=lw, color='r', linestyle='--')\n",
    "    plt.bar(np.arange(benign_log.shape[0]), benign_log*-1, color='navy', label='benign', align='edge')\n",
    "    plt.bar(np.arange(adv_log.shape[0])+ benign_log.shape[0], adv_log*-1, color='darkorange', label='adversarial',\n",
    "            align='edge')\n",
    "    plt.legend()\n",
    "    plt.savefig(folder+attack+'_'+train_or_test+'_histogram.png')\n",
    "    # plt.show(block=True)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonArt",
   "language": "python",
   "name": "art"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
